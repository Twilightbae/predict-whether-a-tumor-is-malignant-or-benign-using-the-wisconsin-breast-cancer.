# -*- coding: utf-8 -*-
"""breast_cancer_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TgaXivnmwHIAQmjAk4M4AeACsABrX65p
"""

# Basic libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Dataset from sklearn
from sklearn.datasets import load_breast_cancer
# Load Wisconsin Breast Cancer dataset
data = load_breast_cancer()

# Features (X) and Target (y)
X = data.data
y = data.target

# Convert to DataFrame
df = pd.DataFrame(X, columns=data.feature_names)
df['target'] = y

# Show first 5 rows
df.head()
print("Dataset shape:", df.shape)
print("\nColumn names:", df.columns.tolist())
print("\nClass distribution:")
print(df['target'].value_counts())

# Replace target values with labels
df['target_label'] = df['target'].map({0: 'Malignant', 1: 'Benign'})
df['target_label'].value_counts()
# Statistical summary of features
df.describe().T
sns.countplot(x='target_label', data=df, palette='pastel')
plt.title("Distribution of Tumor Types")
plt.show()
plt.figure(figsize=(12,10))
sns.heatmap(df.drop('target_label', axis=1).corr(), cmap="coolwarm", annot=False)
plt.title("Correlation Heatmap of Features")
plt.show()
plt.figure(figsize=(12,6))
sns.boxplot(x='target_label', y='mean radius', data=df, palette='Set2')
plt.title("Tumor Radius vs Type")
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(x='target_label', y='mean texture', data=df, palette='Set2')
plt.title("Tumor Texture vs Type")
plt.show()
df.drop('target', axis=1).hist(bins=30, figsize=(20,15), color='skyblue')
plt.suptitle("Feature Distributions", size=20)
plt.show()

# =========================
# PHASE 2: Classical Machine Learning Models
# =========================

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Split dataset
X = df.drop(['target', 'target_label'], axis=1)
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Dictionary to store results
results = {}

# -------------------------
# Logistic Regression
# -------------------------
lr = LogisticRegression(max_iter=5000)
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
results["Logistic Regression"] = accuracy_score(y_test, y_pred_lr)

print("Logistic Regression Report:\n", classification_report(y_test, y_pred_lr))

# -------------------------
# Random Forest
# -------------------------
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
results["Random Forest"] = accuracy_score(y_test, y_pred_rf)

print("Random Forest Report:\n", classification_report(y_test, y_pred_rf))

# -------------------------
# Support Vector Machine
# -------------------------
svm = SVC(probability=True)
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
results["SVM"] = accuracy_score(y_test, y_pred_svm)

print("SVM Report:\n", classification_report(y_test, y_pred_svm))

# -------------------------
# XGBoost
# -------------------------
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
results["XGBoost"] = accuracy_score(y_test, y_pred_xgb)

print("XGBoost Report:\n", classification_report(y_test, y_pred_xgb))

# -------------------------
# Model Comparison
# -------------------------
results_df = pd.DataFrame(list(results.items()), columns=["Model", "Accuracy"])
print(results_df)

sns.barplot(x="Model", y="Accuracy", data=results_df, palette="pastel")
plt.title("Model Accuracy Comparison")
plt.show()

# =========================
# PHASE 3: Neural Network + Explainable AI + Risk Score
# =========================

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd # Import pandas here if it's not imported globally
import numpy as np # Import numpy

# Import LIME
import lime
import lime.lime_tabular

# -------------------------
# 1. Build the Neural Network
# -------------------------
nn_model = keras.Sequential([
    layers.Dense(32, activation="relu", input_shape=(X_train.shape[1],)),
    layers.Dropout(0.3),
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.2),
    layers.Dense(1, activation="sigmoid")  # binary classification
])

nn_model.compile(optimizer="adam",
                 loss="binary_crossentropy",
                 metrics=["accuracy"])

# -------------------------
# 2. Train the Model
# -------------------------
history = nn_model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=16,
    verbose=1
)

# -------------------------
# 3. Evaluate on Test Set
# -------------------------
y_pred_probs = nn_model.predict(X_test).ravel()  # probabilities
y_pred = (y_pred_probs >= 0.5).astype(int)       # threshold 0.5

print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Benign", "Malignant"], yticklabels=["Benign", "Malignant"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Neural Network")
plt.show()

# -------------------------
# 4. Risk Index (0â€“100%)
# -------------------------
risk_scores = (y_pred_probs * 100).round(2)

risk_df = pd.DataFrame({
    "True_Label": y_test.values,
    "Predicted_Label": y_pred,
    "Risk_Score_%": risk_scores
})

print("\nSample Risk Scores:")
print(risk_df.head(10))

# -------------------------
# 5. Explainability with LIME
# -------------------------
# Create a LIME explainer
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Malignant', 'Benign'],
    mode='classification'
)

# Function to get prediction probabilities for LIME
def predict_proba_nn(data):
    probabilities = nn_model.predict(data)
    # LIME expects probabilities for each class.
    # For binary classification with sigmoid output,
    # we can create a 2D array [prob_class_0, prob_class_1]
    return np.hstack((1 - probabilities, probabilities))


# Generate explanations for a few instances in the test set
print("\nLIME Explanations for a few test instances:")
for i in range(5): # Explain first 5 instances
    exp = explainer.explain_instance(
        data_row=X_test.iloc[i].values,
        predict_fn=predict_proba_nn,
        num_features=5 # Show top 5 features
    )
    print(f"\nExplanation for instance {i} (True Label: {y_test.iloc[i]}, Predicted Label: {y_pred[i]}):")
    for feature, weight in exp.as_list():
        print(f"  {feature}: {weight:.4f}")

!pip install lime

# Save trained neural network model
nn_model.save("nn_breast_cancer_model.h5")